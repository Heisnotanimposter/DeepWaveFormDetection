{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "fzpbL2znMdR1",
        "outputId": "794251a4-37f2-4741-9629-b788ffef0cbe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d_P2Y6ykLag7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a580140-8799-4399-e872-452fec6f0af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Real Mel Directory: /content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/train/real\n",
            "Train Fake Mel Directory: /content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/train/fake\n",
            "Contents of Train Real Mel Directory: ['RBYWLNPD.png', 'VWOOFBFB.png', 'HBLWPHKE.png', 'OOWZTSFP.png', 'FEWNBMGJ.png']\n",
            "Contents of Train Fake Mel Directory: ['UADSADXX.png', 'QKQQCYPE.png', 'ZPGBQCZH.png', 'AKIJCVDI.png', 'LCBGBDAQ.png']\n",
            "Real Mel Files: 1000, Fake Mel Files: 1000\n",
            "Mode: train\n",
            "Mel Files: ['/content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/train/real/RBYWLNPD.png', '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/train/real/VWOOFBFB.png', '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/train/real/HBLWPHKE.png', '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/train/real/OOWZTSFP.png', '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/train/real/FEWNBMGJ.png']\n",
            "Labels: [[0, 1], [0, 1], [0, 1], [0, 1], [0, 1]]\n",
            "Mode: test\n",
            "Mel Files: ['/content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/test/TEST_24493.png', '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/test/TEST_23890.png', '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/test/TEST_33227.png', '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/test/TEST_45990.png', '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/test/TEST_30181.png']\n",
            "Training samples: 2000\n",
            "Test samples: 1000\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.689 | Train Acc: 51.56%\n",
            "\t Val. Loss: 0.679 |  Val. Acc: 52.00%\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.628 | Train Acc: 68.62%\n",
            "\t Val. Loss: 0.598 |  Val. Acc: 72.25%\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.558 | Train Acc: 71.62%\n",
            "\t Val. Loss: 0.515 |  Val. Acc: 77.25%\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.456 | Train Acc: 78.69%\n",
            "\t Val. Loss: 0.447 |  Val. Acc: 84.00%\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.392 | Train Acc: 82.19%\n",
            "\t Val. Loss: 0.401 |  Val. Acc: 84.50%\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.335 | Train Acc: 85.62%\n",
            "\t Val. Loss: 0.363 |  Val. Acc: 86.00%\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.284 | Train Acc: 88.50%\n",
            "\t Val. Loss: 0.341 |  Val. Acc: 86.75%\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.263 | Train Acc: 89.56%\n",
            "\t Val. Loss: 0.328 |  Val. Acc: 88.50%\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.274 | Train Acc: 87.50%\n",
            "\t Val. Loss: 0.326 |  Val. Acc: 89.00%\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.239 | Train Acc: 90.75%\n",
            "\t Val. Loss: 0.305 |  Val. Acc: 89.25%\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.212 | Train Acc: 91.31%\n",
            "\t Val. Loss: 0.272 |  Val. Acc: 90.75%\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.196 | Train Acc: 92.56%\n",
            "\t Val. Loss: 0.300 |  Val. Acc: 89.75%\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.189 | Train Acc: 92.69%\n",
            "\t Val. Loss: 0.260 |  Val. Acc: 92.00%\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.193 | Train Acc: 92.38%\n",
            "\t Val. Loss: 0.250 |  Val. Acc: 92.00%\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.153 | Train Acc: 94.88%\n",
            "\t Val. Loss: 0.229 |  Val. Acc: 92.25%\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.153 | Train Acc: 94.00%\n",
            "\t Val. Loss: 0.232 |  Val. Acc: 93.75%\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.142 | Train Acc: 95.00%\n",
            "\t Val. Loss: 0.265 |  Val. Acc: 91.25%\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.134 | Train Acc: 95.88%\n",
            "\t Val. Loss: 0.218 |  Val. Acc: 93.50%\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.118 | Train Acc: 95.62%\n",
            "\t Val. Loss: 0.221 |  Val. Acc: 93.50%\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.127 | Train Acc: 94.94%\n",
            "\t Val. Loss: 0.239 |  Val. Acc: 92.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Loss: 0.218 | Final Acc: 93.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file created successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    SR = 32000\n",
        "    N_MELS = 128\n",
        "    N_MFCC = 13\n",
        "    MAX_SEQ_LEN = 200\n",
        "    ROOT_FOLDER = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/preprocessed/'\n",
        "    BATCH_SIZE = 64\n",
        "    N_EPOCHS = 20\n",
        "    LR = 1e-4\n",
        "    SUBSET_SIZE = 1000\n",
        "\n",
        "CONFIG = Config()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Custom Dataset for Mel-spectrogram images\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, mel_files, labels=None, transform=None):\n",
        "        self.mel_files = mel_files\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mel_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mel_image = Image.open(self.mel_files[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            mel_image = self.transform(mel_image)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "            return mel_image, label\n",
        "        return mel_image\n",
        "\n",
        "# Load file paths and labels for the datasets\n",
        "def load_file_paths_and_labels(root_folder, subset_size=CONFIG.SUBSET_SIZE, mode='train'):\n",
        "    if mode == 'train':\n",
        "        real_mel_files = glob.glob(os.path.join(root_folder, 'train', 'real', '*.png'))[:subset_size]\n",
        "        fake_mel_files = glob.glob(os.path.join(root_folder, 'train', 'fake', '*.png'))[:subset_size]\n",
        "\n",
        "        mel_files = real_mel_files + fake_mel_files\n",
        "        labels = [[0, 1]] * len(real_mel_files) + [[1, 0]] * len(fake_mel_files)\n",
        "\n",
        "        print(f\"Real Mel Files: {len(real_mel_files)}, Fake Mel Files: {len(fake_mel_files)}\")\n",
        "    else:\n",
        "        mel_files = glob.glob(os.path.join(root_folder, 'test', '*.png'))[:subset_size]\n",
        "        labels = None\n",
        "\n",
        "    print(f\"Mode: {mode}\")\n",
        "    print(f\"Mel Files: {mel_files[:5]}\")\n",
        "    if mode == 'train':\n",
        "        print(f\"Labels: {labels[:5]}\")\n",
        "\n",
        "    return mel_files, labels\n",
        "\n",
        "# Verify the directory structure and files\n",
        "def verify_directories(root_folder):\n",
        "    try:\n",
        "        train_real_mel_dir = os.path.join(root_folder, 'train', 'real')\n",
        "        train_fake_mel_dir = os.path.join(root_folder, 'train', 'fake')\n",
        "\n",
        "        print(f\"Train Real Mel Directory: {train_real_mel_dir}\")\n",
        "        print(f\"Train Fake Mel Directory: {train_fake_mel_dir}\")\n",
        "\n",
        "        print(\"Contents of Train Real Mel Directory:\", os.listdir(train_real_mel_dir)[:5])\n",
        "        print(\"Contents of Train Fake Mel Directory:\", os.listdir(train_fake_mel_dir)[:5])\n",
        "    except (OSError, IOError) as e:\n",
        "        print(f\"Error accessing directories: {e}\")\n",
        "\n",
        "verify_directories(CONFIG.ROOT_FOLDER)\n",
        "\n",
        "# Load file paths and labels\n",
        "train_mel_files, train_labels = load_file_paths_and_labels(CONFIG.ROOT_FOLDER, mode='train')\n",
        "test_mel_files, _ = load_file_paths_and_labels(CONFIG.ROOT_FOLDER, mode='test')\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f'Training samples: {len(train_mel_files)}')\n",
        "print(f'Test samples: {len(test_mel_files)}')\n",
        "\n",
        "# Ensure non-empty loaders\n",
        "assert len(train_mel_files) > 0, \"Training dataset is empty!\"\n",
        "assert len(test_mel_files) > 0, \"Test dataset is empty!\"\n",
        "\n",
        "# Data transformations for Mel-spectrogram images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "full_dataset = CustomDataset(train_mel_files, train_labels, transform=transform)\n",
        "test_dataset = CustomDataset(test_mel_files, transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Define the BiLSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        num_directions = 2 if self.lstm.bidirectional else 1\n",
        "        h_0 = torch.zeros(self.lstm.num_layers * num_directions, x.size(0), self.lstm.hidden_size).to(device)\n",
        "        c_0 = torch.zeros(self.lstm.num_layers * num_directions, x.size(0), self.lstm.hidden_size).to(device)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, _ = self.lstm(x, (h_0, c_0))\n",
        "        x = self.fc(self.dropout(lstm_out[:, -1, :]))\n",
        "        return x\n",
        "\n",
        "# Define the CNN model for Mel-spectrogram images\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(128 * 16 * 16, 256)\n",
        "        self.fc2 = nn.Linear(256, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.gelu(self.conv1(x)))\n",
        "        x = self.pool(F.gelu(self.conv2(x)))\n",
        "        x = self.pool(F.gelu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 16 * 16)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Combine both models\n",
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, lstm_input_dim, lstm_hidden_dim, lstm_output_dim, lstm_n_layers, lstm_bidirectional, lstm_dropout, cnn_output_dim):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.lstm = BiLSTM(lstm_input_dim, lstm_hidden_dim, lstm_output_dim, lstm_n_layers, lstm_bidirectional, lstm_dropout)\n",
        "        self.cnn = CNN(cnn_output_dim)\n",
        "        self.fc = nn.Linear(lstm_output_dim + cnn_output_dim, 2)\n",
        "\n",
        "    def forward(self, mfcc, mel):\n",
        "        if mfcc is not None:\n",
        "            mfcc = mfcc.permute(0, 2, 1)\n",
        "            lstm_out = self.lstm(mfcc)\n",
        "        else:\n",
        "            lstm_out = torch.zeros(mel.size(0), 128).to(device)\n",
        "\n",
        "        cnn_out = self.cnn(mel)\n",
        "        combined = torch.cat((lstm_out, cnn_out), dim=1)\n",
        "        out = self.fc(combined)\n",
        "        return out\n",
        "\n",
        "# Model initialization\n",
        "model = CombinedModel(\n",
        "    lstm_input_dim=CONFIG.N_MFCC,\n",
        "    lstm_hidden_dim=128,\n",
        "    lstm_output_dim=128,\n",
        "    lstm_n_layers=2,\n",
        "    lstm_bidirectional=True,\n",
        "    lstm_dropout=0.5,\n",
        "    cnn_output_dim=256\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=CONFIG.LR)\n",
        "\n",
        "# Training and validation functions\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for mel, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        mel, labels = mel.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(None, mel)  # Only use Mel-spectrogram\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, labels = torch.max(labels.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total if total > 0 else 0  # Avoid division by zero\n",
        "    return epoch_loss / len(loader) if len(loader) > 0 else 0, accuracy\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for mel, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            mel, labels = mel.to(device), labels.to(device)\n",
        "            outputs = model(None, mel)\n",
        "            loss = criterion(outputs, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            _, labels = torch.max(labels.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total if total > 0 else 0  # Avoid division by zero\n",
        "    return epoch_loss / len(loader) if len(loader) > 0 else 0, accuracy\n",
        "\n",
        "# Training loop\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(CONFIG.N_EPOCHS):\n",
        "    print(f'Epoch {epoch+1}/{CONFIG.N_EPOCHS}')\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    valid_loss, valid_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "# Final evaluation on the validation set\n",
        "final_loss, final_acc = evaluate(model, val_loader, criterion, device)\n",
        "print(f'Final Loss: {final_loss:.3f} | Final Acc: {final_acc*100:.2f}%')\n",
        "\n",
        "# Prediction on test dataset\n",
        "def predict(model, loader, device):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for mel in tqdm(loader, desc=\"Predicting\", leave=False):\n",
        "            mel = mel.to(device)\n",
        "            outputs = model(None, mel)\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            all_predictions.extend(probs.cpu().numpy())\n",
        "    return np.array(all_predictions)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = predict(model, test_loader, device)\n",
        "\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame(test_predictions, columns=['fake', 'real'])\n",
        "\n",
        "# Extracting IDs from test file paths\n",
        "test_ids = [os.path.basename(f).replace('.png', '') for f in test_mel_files]\n",
        "submission_df.insert(0, 'id', test_ids)\n",
        "\n",
        "# Save to CSV\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "print('Submission file created successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced training and evaluation functions with debug prints\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (mel, labels) in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
        "        mel, labels = mel.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(None, mel)  # Only use Mel-spectrogram\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, labels = torch.max(labels.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Debug prints for each batch\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Batch {batch_idx}/{len(loader)}: Loss = {loss.item()}, Accuracy = {correct / total * 100:.2f}%\")\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0  # Avoid division by zero\n",
        "    return epoch_loss / len(loader) if len(loader) > 0 else 0, accuracy\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (mel, labels) in enumerate(tqdm(loader, desc=\"Evaluating\", leave=False)):\n",
        "            mel, labels = mel.to(device), labels.to(device)\n",
        "            outputs = model(None, mel)  # Only use Mel-spectrogram\n",
        "            loss = criterion(outputs, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            _, labels = torch.max(labels.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Debug prints for each batch\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Batch {batch_idx}/{len(loader)}: Loss = {loss.item()}, Accuracy = {correct / total * 100:.2f}%\")\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0  # Avoid division by zero\n",
        "    return epoch_loss / len(loader) if len(loader) > 0 else 0, accuracy"
      ],
      "metadata": {
        "id": "Vc4szSlcbuAt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}