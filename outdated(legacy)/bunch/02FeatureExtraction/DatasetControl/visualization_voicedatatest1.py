# -*- coding: utf-8 -*-
"""visualization_VoicedataTest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mAMqXtDcl1VAThug50PNMs20mwuWKrmq
"""

from google.colab import drive
drive.mount('/content/drive')

#libraries
import csv
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.preprocessing import LabelEncoder

# dataset
chunk_size = 50000
#Number of Rows per Chunk
chunks = pd.read_csv('/content/drive/MyDrive/dataset/TeamDeepwave/dataset/combined_file.csv', low_memory = False, chunksize = chunk_size)

for i, chunks in enumerate(chunks):
  chunks.to_csv(f'chunks_{i}.csv', index = False)

dataset_path = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/cv-corpus-17.0-delta-2024-03-15/en'
all_filenames = glob.glob(dataset_path + '/*.tsv')
dataframes = []

for filename in all_filenames:
    df = pd.read_csv(filename, sep='\t', encoding='utf-8')
    dataframes.append(df)
    metadata_csv
 = pd.concat(dataframes)
    metadata_csv
.to_csv('/content/drive/MyDrive/dataset/TeamDeepwave/dataset/combined_file.csv', index=False, encoding='utf-8-sig')

print(metadata_csv.head(10))

dropna_csv = metadata_csv.dropna()

cleansed_csv = metadata_csv.dropna(how = 'all')
print("DataFrame with rows dropped where any column has NaN:")
print(dropna_csv.head(10))

print("\nDataFrame with rows dropped where all columns are NaN:")
print(cleansed_csv.head(10))

tsv_read = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/cv-corpus-17.0-delta-2024-03-15/en/validated.tsv'
tsv_filename = glob.glob(tsv_read + '/*.tsv')

for filename in tsv_filename:
    df = pd.read_csv(filename, sep='\t', encoding='utf-8')

print(f"Inspecting{filename}:")
print(df.head(10))
print(df.shape)
print(df.info())

df['sentence_domain'] = df['sentence_domain'].fillna('Unknown')
df_cleaned = df.dropna()
print(metadata_csv.columns)

# Assuming 'label' is the gender column and other columns are acoustic features
X = metadata_csv.drop(columns=['clips_count'])
y = metadata_csv['client_id']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Metric function
def print_metrics(y_true, y_pred):
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precision: {precision_score(y_true, y_pred, average='weighted'):.4f}")
    print(f"Recall: {recall_score(y_true, y_pred, average='weighted'):.4f}")
    print(f"F1 Score: {f1_score(y_true, y_pred, average='weighted'):.4f}")

from sklearn.preprocessing import LabelEncoder
from sklearn.exceptions import NotFittedError

class ExtendedLabelEncoder(LabelEncoder):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.classes_ = None

    def fit(self, y):
        super().fit(y)
        self.classes_ = super().classes_

    def transform(self, y):
        try:
            return super().transform(y)
        except NotFittedError:
            raise
        except ValueError as e:
            # Assign a default value for unseen labels
            unseen_label = max(self.classes_) + 1
            self.classes_ = np.append(self.classes_, unseen_label)
            return np.where(y == e.args[0], unseen_label, super().transform(y))

# Initialize the extended label encoder
extended_label_encoder = ExtendedLabelEncoder()

# Fit and transform the 'sentence_id' column in the training set
X_train['sentence_id'] = extended_label_encoder.fit_transform(X_train['sentence_id'])

# Transform the 'sentence_id' column in the test set
# This will handle unseen labels by assigning them a default value
X_test['sentence_id'] = extended_label_encoder.transform(X_test['sentence_id'])

# Now you can fit the DecisionTreeClassifier and predict
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

# Assuming you have a function 'print_metrics' to print the evaluation metrics
print("Decision Tree Classifier Metrics:")
print_metrics(y_test, y_pred_dt)

# 2) LogisticRegression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print("\nLogistic Regression Metrics:")
print_metrics(y_test, y_pred_lr)

# 3) Support Vector Machine
svc = SVC()
svc.fit(X_train, y_train)
y_pred_svc = svc.predict(X_test)
print("\nSupport Vector Machine Metrics:")
print_metrics(y_test, y_pred_svc)

# 4) K-Nearest Neighbors
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
print("\nK-Nearest Neighbors Metrics:")
print_metrics(y_test, y_pred_knn)

# 5) Random Forest
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("\nRandom Forest Metrics:")
print_metrics(y_test, y_pred_rf)