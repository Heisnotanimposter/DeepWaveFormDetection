import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Reshape
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from tensorflow.keras.callbacks import TensorBoard
import datetime

# Define paths
real_dataset_path = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/KaggleDataset/real'
fake_dataset_path = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/KaggleDataset/fake'

# Load metadata
metadata = pd.read_csv('/content/drive/MyDrive/dataset/TeamDeepwave/dataset/metadata_file.csv')

# Assume the metadata contains paths to audio files and precomputed MFCCs
def load_precomputed_features(metadata):
    features = []
    labels = []
    for i, row in metadata.iterrows():
        # Assuming the metadata has 'mfcc' column with precomputed features and 'label' column for real/fake
        mfcc = np.array(eval(row['mfcc']))  # Convert string representation of list to numpy array
        features.append(mfcc)
        labels.append(row['label'])
    return np.array(features), np.array(labels)

# Load features and labels
X, y = load_precomputed_features(metadata)

# Ensure all feature arrays are the same length (e.g., pad sequences)
max_len = max([x.shape[1] for x in X])
X = np.array([np.pad(x, ((0, 0), (0, max_len - x.shape[1])), mode='constant') for x in X])

# Reshape for Conv2D
X = X[..., np.newaxis]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define CNN model
cnn_model = Sequential([
    Reshape((X_train.shape[1], X_train.shape[2], 1), input_shape=X_train.shape[1:]),
    Conv2D(32, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

cnn_model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])

# TensorBoard callback
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

# Train CNN model
cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])

# Evaluate CNN model
cnn_model.evaluate(X_test, y_test)

# Predictions and metrics
y_pred_prob = cnn_model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype("int32")

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Confusion Matrix:\n {conf_matrix}")

plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Real", "Fake"], yticklabels=["Real", "Fake"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

# XGBoost Classifier with Grid Search
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

xgb = XGBClassifier()
grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2)
grid_search_xgb.fit(X_train, y_train)

# LightGBM Classifier with Random Search
lgbm = LGBMClassifier()
random_search_lgbm = RandomizedSearchCV(estimator=lgbm, param_distributions=param_grid, n_iter=10, cv=3, scoring='accuracy', verbose=2)
random_search_lgbm.fit(X_train, y_train)

# Best parameters
print("Best parameters for XGBoost:", grid_search_xgb.best_params_)
print("Best parameters for LightGBM:", random_search_lgbm.best_params_)

# Evaluate the best models
best_xgb = grid_search_xgb.best_estimator_
best_lgbm = random_search_lgbm.best_estimator_

def evaluate_model(model, X_train, X_test, y_train, y_test, label):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    
    print(f"\n{label} Model Metrics:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"Confusion Matrix:\n {conf_matrix}")
    
    plt.figure(figsize=(10, 7))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Real", "Fake"], yticklabels=["Real", "Fake"])
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"Confusion Matrix - {label}")
    plt.show()

# Evaluate XGBoost and LightGBM models
evaluate_model(best_xgb, X_train, X_test, y_train, y_test, "XGBoost")
evaluate_model(best_lgbm, X_train, X_test, y_train, y_test, "LightGBM")

# TensorBoard
%load_ext tensorboard
%tensorboard --logdir logs/fit