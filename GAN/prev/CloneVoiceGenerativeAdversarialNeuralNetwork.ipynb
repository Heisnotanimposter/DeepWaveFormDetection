{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "real_dataset_path = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/KaggleDataset/real'\n",
        "fake_dataset_path = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/KaggleDataset/fake'\n",
        "\n",
        "# Function to load audio and create spectrograms\n",
        "# Function to load audio and create padded spectrograms\n",
        "def load_audio(file_path, sr=22050, max_length=128):\n",
        "    y, _ = librosa.load(file_path, sr=sr)\n",
        "    stft = librosa.stft(y)\n",
        "    spectrogram = librosa.amplitude_to_db(np.abs(stft), ref=np.max)\n",
        "\n",
        "    # Pad spectrograms to have the same length\n",
        "    if spectrogram.shape[1] < max_length:\n",
        "        padding = max_length - spectrogram.shape[1]\n",
        "        spectrogram = np.pad(spectrogram, pad_width=((0, 0), (0, padding)), mode='constant')\n",
        "    else:\n",
        "        spectrogram = spectrogram[:, :max_length]\n",
        "    return spectrogram\n",
        "\n",
        "# Load real spectrograms with padding\n",
        "real_spectrograms = []\n",
        "for file in glob.glob(real_dataset_path + '/*.wav'):\n",
        "    real_spectrograms.append(load_audio(file))\n",
        "real_spectrograms = np.array(real_spectrograms)\n",
        "\n",
        "# Load real spectrograms\n",
        "real_spectrograms = []\n",
        "for file in glob.glob(real_dataset_path + '/*.wav'):\n",
        "    real_spectrograms.append(load_audio(file))\n",
        "real_spectrograms = np.array(real_spectrograms)\n",
        "real_labels = np.ones(len(real_spectrograms))  # Labels for real spectrograms\n",
        "\n",
        "# Load fake spectrograms\n",
        "fake_spectrograms = []\n",
        "for file in glob.glob(fake_dataset_path + '/*.wav'):\n",
        "    fake_spectrograms.append(load_audio(file))\n",
        "fake_spectrograms = np.array(fake_spectrograms)\n",
        "fake_labels = np.zeros(len(fake_spectrograms))  # Labels for fake spectrograms\n",
        "\n",
        "# Combine and shuffle data\n",
        "all_spectrograms = np.concatenate([real_spectrograms, fake_spectrograms])\n",
        "all_labels = np.concatenate([real_labels, fake_labels])\n",
        "\n",
        "# Shuffle data and labels together\n",
        "shuffle_indices = np.random.permutation(len(all_spectrograms))\n",
        "all_spectrograms = all_spectrograms[shuffle_indices]\n",
        "all_labels = all_labels[shuffle_indices]\n",
        "\n",
        "# Normalize and resize spectrograms (adjust size as needed)\n",
        "all_spectrograms = (all_spectrograms - np.min(all_spectrograms)) / (np.max(all_spectrograms) - np.min(all_spectrograms))\n",
        "all_spectrograms = np.array([cv2.resize(spec, (128, 128)) for spec in all_spectrograms])\n",
        "\n",
        "# Convert to PyTorch tensors and create DataLoader\n",
        "all_spectrograms_tensor = torch.tensor(all_spectrograms, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
        "all_labels_tensor = torch.tensor(all_labels, dtype=torch.float32).unsqueeze(1)\n",
        "dataset = torch.utils.data.TensorDataset(all_spectrograms_tensor, all_labels_tensor)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define GAN Architecture\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim=100, output_channels=1, image_size=128):\n",
        "        super(Generator, self).__init__()\n",
        "        self.output_channels = output_channels  # Store output_channels as an attribute\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(input_dim, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(64, self.output_channels, 4, 2, 1, bias=False), # Use self.output_channels\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_channels=1, image_size=128):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(input_channels, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * 8 * 8, 1024),  # Hidden layer      #nn.Linear(512 * 4 * 4, 1024)\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 1),              # Output layer\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Training Loop\n",
        "def train_gan(generator, discriminator, dataloader, num_epochs=100, lr=0.0002, device=\"cuda\"):  # Default to 'cuda'\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    # Fixed noise for visualization\n",
        "    fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(dataloader):\n",
        "            # Ensure models and data are on the correct device\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            generator.to(device)\n",
        "            discriminator.to(device)\n",
        "\n",
        "            # Train Discriminator with all-real batch\n",
        "            discriminator.zero_grad()\n",
        "            output = discriminator(images).view(-1)\n",
        "            errD_real = criterion(output, labels)\n",
        "            errD_real.backward()\n",
        "\n",
        "            # Train Discriminator with all-fake batch\n",
        "            noise = torch.randn(images.size(0), 100, 1, 1, device=device)\n",
        "            fake = generator(noise)\n",
        "            output = discriminator(fake.detach()).view(-1)\n",
        "            errD_fake = criterion(output, torch.zeros_like(labels))\n",
        "            errD_fake.backward()\n",
        "\n",
        "            errD = errD_real + errD_fake\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Train Generator\n",
        "            generator.zero_grad()\n",
        "            output = discriminator(fake).view(-1)\n",
        "            errG = criterion(output, labels)\n",
        "            errG.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # Output training stats\n",
        "            if i % 50 == 0:\n",
        "                print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}]\\tLoss_D: {errD.item():.4f}\\tLoss_G: {errG.item():.4f}')\n",
        "    return generator\n",
        "\n",
        "# Initialize models and determine device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the creation of dataloader outside the train_gan function\n",
        "# Convert to PyTorch tensors and create DataLoader\n",
        "all_spectrograms_tensor = torch.tensor(all_spectrograms, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
        "all_labels_tensor = torch.tensor(all_labels, dtype=torch.float32).unsqueeze(1)\n",
        "dataset = torch.utils.data.TensorDataset(all_spectrograms_tensor, all_labels_tensor)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Now, call train_gan function with dataloader\n",
        "generator = train_gan(generator, discriminator, dataloader, device=device)  # Pass device to train_gan\n",
        "\n",
        "\n",
        "# Generate spectrograms\n",
        "generated_spectrograms = generator(fixed_noise).detach().cpu()\n",
        "\n",
        "# Visualize generated spectrograms (only first 9 for demonstration)\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(generated_spectrograms[i].squeeze(0), cmap='viridis', origin='lower', aspect='auto')  # Assuming the spectrogram is the first channel\n",
        "    plt.title(f'Generated Spectrogram {i + 1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n",
        "# Move the creation of dataloader outside the train_gan function\n",
        "# Convert to PyTorch tensors and create DataLoader\n",
        "all_spectrograms_tensor = torch.tensor(all_spectrograms, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
        "all_labels_tensor = torch.tensor(all_labels, dtype=torch.float32).unsqueeze(1)\n",
        "dataset = torch.utils.data.TensorDataset(all_spectrograms_tensor, all_labels_tensor)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Now, call train_gan function with dataloader\n",
        "train_gan(generator, discriminator, dataloader, device=device)  # Pass device to train_gan\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "2wWRT7AiFEVz",
        "outputId": "b0c67414-ecb0-4e00-e884-319bc69ed883"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])) is deprecated. Please ensure they have the same size.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1433971fc02f>\u001b[0m in \u001b[0;36m<cell line: 193>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;31m# Now, call train_gan function with dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass device to train_gan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-1433971fc02f>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, dataloader, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0merrD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0merrD_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3143\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3145\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   3146\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3147\u001b[0m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])) is deprecated. Please ensure they have the same size."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9AR4S0nmFxNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}